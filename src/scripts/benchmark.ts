#!/usr/bin/env node
// Performance benchmark CLI script per PERF-001 rules
import { promises as fs } from "fs";
import path from "path";
import { createBenchmarker } from "../benchmarks/performance.js";

interface BenchmarkConfig {
  testRepos: Array<{
    path: string;
    name: string;
    expectedSize?: "small" | "medium" | "large";
  }>;
  outputDir?: string;
  verbose?: boolean;
}

async function main() {
  const args = process.argv.slice(2);
  const command = args[0] || "help";

  switch (command) {
    case "run":
      await runBenchmarks(args.slice(1));
      break;
    case "current":
      await benchmarkCurrentRepo();
      break;
    case "create-config":
      await createDefaultConfig();
      break;
    case "help":
    default:
      printHelp();
      break;
  }
}

async function runBenchmarks(args: string[]) {
  const configPath = args[0] || "./benchmark-config.json";

  try {
    const configContent = await fs.readFile(configPath, "utf-8");
    const config: BenchmarkConfig = JSON.parse(configContent);

    console.log("üéØ Performance Benchmarking System (PERF-001 Compliance)");
    console.log("Target Performance:");
    console.log("  ‚Ä¢ Small repos (<100 files): <1 second");
    console.log("  ‚Ä¢ Medium repos (100-1000 files): <10 seconds");
    console.log("  ‚Ä¢ Large repos (1000+ files): <60 seconds\\n");

    const benchmarker = createBenchmarker();
    const suite = await benchmarker.runBenchmarkSuite(config.testRepos);

    // Print detailed report
    benchmarker.printDetailedReport(suite);

    // Export results if output directory specified
    if (config.outputDir) {
      await fs.mkdir(config.outputDir, { recursive: true });
      const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
      const outputPath = path.join(
        config.outputDir,
        `benchmark-${timestamp}.json`,
      );

      await benchmarker.exportResults(suite, outputPath);
      console.log(`\\nüìÑ Results exported to: ${outputPath}`);
    }

    // Exit with appropriate code
    process.exit(suite.overallPassed ? 0 : 1);
  } catch (error) {
    console.error("‚ùå Benchmark failed:", error);
    console.error(
      '\\nTry running "npm run benchmark:create-config" to create a default configuration.',
    );
    process.exit(1);
  }
}

async function benchmarkCurrentRepo() {
  console.log("üéØ Benchmarking Current Repository");
  console.log("=".repeat(40));

  const currentRepo = process.cwd();
  const repoName = path.basename(currentRepo);

  const benchmarker = createBenchmarker();

  try {
    console.log(`üìä Analyzing: ${repoName} at ${currentRepo}\\n`);

    const result = await benchmarker.benchmarkRepository(
      currentRepo,
      "standard",
    );

    // Generate single-repo suite
    const suite = benchmarker.generateSuite(`Current Repository: ${repoName}`, [
      result,
    ]);

    // Print results
    benchmarker.printDetailedReport(suite);

    // Export to current directory
    const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
    const outputPath = `./benchmark-current-${timestamp}.json`;
    await benchmarker.exportResults(suite, outputPath);

    console.log(`\\nüìÑ Results saved to: ${outputPath}`);

    process.exit(suite.overallPassed ? 0 : 1);
  } catch (error) {
    console.error("‚ùå Benchmark failed:", error);
    process.exit(1);
  }
}

async function createDefaultConfig() {
  const defaultConfig: BenchmarkConfig = {
    testRepos: [
      {
        path: ".",
        name: "Current Repository",
        expectedSize: "small",
      },
      // Add more test repositories here
      // {
      //   path: "/path/to/medium/repo",
      //   name: "Medium Test Repo",
      //   expectedSize: "medium"
      // },
      // {
      //   path: "/path/to/large/repo",
      //   name: "Large Test Repo",
      //   expectedSize: "large"
      // }
    ],
    outputDir: "./benchmark-results",
    verbose: true,
  };

  const configPath = "./benchmark-config.json";
  await fs.writeFile(configPath, JSON.stringify(defaultConfig, null, 2));

  console.log("‚úÖ Created default benchmark configuration:");
  console.log(`   ${configPath}`);
  console.log("");
  console.log("üìù Edit this file to add your test repositories, then run:");
  console.log("   npm run benchmark:run");
}

function printHelp() {
  console.log("üéØ DocuMCP Performance Benchmarking Tool");
  console.log("");
  console.log("USAGE:");
  console.log(
    "  npm run benchmark:run [config-file]     Run full benchmark suite",
  );
  console.log(
    "  npm run benchmark:current               Benchmark current repository only",
  );
  console.log(
    "  npm run benchmark:create-config         Create default configuration",
  );
  console.log("  npm run benchmark:help                  Show this help");
  console.log("");
  console.log("PERFORMANCE TARGETS (PERF-001):");
  console.log("  ‚Ä¢ Small repositories (<100 files): <1 second");
  console.log("  ‚Ä¢ Medium repositories (100-1000 files): <10 seconds");
  console.log("  ‚Ä¢ Large repositories (1000+ files): <60 seconds");
  console.log("");
  console.log("EXAMPLES:");
  console.log("  npm run benchmark:current");
  console.log("  npm run benchmark:create-config");
  console.log("  npm run benchmark:run ./my-config.json");
}

// Handle unhandled promise rejections
process.on("unhandledRejection", (error) => {
  console.error("‚ùå Unhandled rejection:", error);
  process.exit(1);
});

main().catch((error) => {
  console.error("‚ùå Script failed:", error);
  process.exit(1);
});
